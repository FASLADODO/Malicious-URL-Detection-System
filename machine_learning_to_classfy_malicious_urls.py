# -*- coding: utf-8 -*-
"""Machine learning to classfy malicious urls

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p7vcJ_ASnJxYWps6LKChwzaCiyUBg988
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'malicious-urls-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1486586%2F2456026%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240514%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240514T071922Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D81ee9e68c5ebfd0d82da62d09213f095f69910c2ef5b2f06a425690027e5d47ff76d37068c1d062eea90a66582eb8eef6284f0c67faf6c24d2e0ed9a660fb1b7ea0afab4c0a75ab9aa0e729328f5abfa700775998b1cab8b84ab4f09e4f944cc49440f1918c13d6df262c38a49baa2174f9b10347a1b247124f8321825d54decc5c7b01200e57c1d6799dc293da3e5d9e8719befb76aff990d618b85dede3a786ceada34eed4cd47fb296edb4fcea88c26b733df6e7c258759ce8ea2a5a12d339ac68ebae2b57ca000d2a836df9022a7927b027d9f68e426a24992f8e9308295bdf5e76519a346e0dd54c62e5c286bd8cfb7a88b43fcc61baedffbb36b776335'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

"""<div style="text-align: center; font-size: 24px; font-weight: bold; color: blue;">
    <p style="font-size: 36px; font-weight: 800;">Detecting Malicious URLs Using Machine Learning</p>
</div>
Welcome to my Kaggle notebook. In this comprehensive analysis, I delve into the essential features that play a pivotal role in identifying potentially harmful URLs.

In this notebook, I focus on dissecting and understanding various features that contribute to the identification of malicious URLs. The features under scrutiny include:

**URL Length**: A fundamental metric that often distinguishes legitimate URLs from malicious ones. Long, convoluted URLs might indicate attempts to obfuscate their true intent.

**Domain Abnormality**: Analyzing the structural peculiarities of domains can provide valuable insights. Suspicious deviations from typical domain patterns could signify malicious intent.

**JavaScript Code**: Detecting the presence of JavaScript code, a popular tool for implementing various types of attacks.

**Abnormal URL Components**: Exploring unconventional URL components that might suggest attempts to deceive security mechanisms.

**HTTPS Presence**: The presence or absence of HTTPS in a URL can offer clues about its security. Malicious actors often avoid using HTTPS to appear more legitimate.

**Text Encoding**: Encoding the text in the link, such as URL encoding or converting characters into symbols, can affect the behavior and security of the link.

**Digit Count**: Quantifying the ratio of digits in a URL can aid in identifying patterns associated with phishing or fraudulent URLs.

**Letter Count**: Similar to digit count, evaluating the proportion of letters in URLs contributes to the overall analysis.

**Shortening Service**: Investigating the use of URL shortening services, which can obscure the actual destination and potentially lead to malicious content.

**IP Address Presence**: Identifying URLs that use IP addresses instead of domain names, a common strategy among malicious URLs.


Through detailed exploratory data analysis of these features, I aim to shed light on their significance in classifying URLs as malicious or benign. By leveraging machine learning techniques, I endeavor to build predictive models that can effectively discern between safe and harmful URLs, contributing to the ongoing efforts to enhance internet security.
"""

pip install tldextract

pip install optuna

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import tldextract
from urllib.parse import urlparse
import re
from sklearn.model_selection import train_test_split
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from urllib.parse import urlparse, unquote

from google.colab import drive
 drive.mount('/content/drive')

 import os
 print(os.listdir('/content/drive/MyDrive/Colab Notebooks/malicious URL dataset'))

# import os
# print(os.listdir('/kaggle/input/'))

# Define the file path
 file_path = '/content/drive/MyDrive/Colab Notebooks/malicious URL dataset/malicious_phish.csv'

# Load the dataset into a Pandas DataFrame
 data = pd.read_csv(file_path)

data.head() # Display the first few rows of the DataFrame

data.shape

data.isnull().sum()

# Count the number of values for each 'type'
type_counts = data['type'].value_counts()
type_counts

# Assuming you have type_counts as a pandas Series containing the count of each type
plt.figure(figsize=(6, 6))
plt.pie(type_counts, labels=type_counts.index, autopct='%1.1f%%', colors=sns.color_palette("pastel"))
plt.title('Distribution of Urls')
plt.show()

# Remove "www." from URLs and update the 'url' column
data['url'] = data['url'].str.replace('www.', '')

# Print the first few rows of the updated DataFrame
data.head()

# Create a dictionary to map types to categories
type_to_category = {
    "benign": 0,
    "defacement": 1,
    "phishing": 2,
    "malware": 3
}

# Add a new 'Category' column based on the 'type' column
data['Category'] = data['type'].map(type_to_category)

data.head()

# Add a new 'URL_Length' column with the length of each URL
data['URL_Length'] = data['url'].apply(len)

# Create a scatter plot using Seaborn
plt.figure(figsize=(10, 6))
sns.scatterplot(data=data, x='type', y='URL_Length')
plt.title("Relation between URL Length and Type")
plt.xlabel("Type")
plt.ylabel("URL Length")
plt.tight_layout()
plt.show()

# Create a bar plot using Seaborn
plt.figure(figsize=(10, 6))
sns.barplot(data=data, x='type', y='URL_Length',palette="bright", ci=None)
plt.title("Relationship between URL Length and Type")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Calculate the ratios for each URL type within the same type
data['type_ratio'] = data.groupby('type')['URL_Length'].transform(lambda x: x / x.sum() * 100)

# Create a bar plot using Seaborn
plt.figure(figsize=(10, 6))
sns.barplot(data=data, x='type', y='type_ratio', ci=None, palette='bright')
plt.title("URL Length Ratio for Each Type")
plt.ylabel("Type Ratio (%)")
plt.xlabel("URL Type")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

data.head()

# Function to extract domain from URL using tldextract
def extract_domain(url):
    extracted = tldextract.extract(url)
    return f"{extracted.domain}.{extracted.suffix}"

# Add a new 'domain' column with the extracted domain from each URL
data['domain'] = data['url'].apply(extract_domain)

data.head()

# List of characters to count
characters_to_count = ['@', '?', '-', '=', '.', '#', '%', '+', '$', '!', '*', ',', '//']

# Function to count occurrences of a character in a URL
def count_character_occurrences(url, character):
    return url.count(character)

# Add new columns for each character and count occurrences
for character in characters_to_count:
    data[f'{character}'] = data['url'].apply(lambda url: count_character_occurrences(url, character))

data.head()

# Function to detect if there's a match between URL and host
def abnormal_url(url):
    hostname = urlparse(url).hostname
    hostname = str(hostname)
    match = re.search(hostname, url)
    if match:
        return 1
    else:
        return 0

# Add a new 'Abnormal_URL' column with 1 if there's a match, 0 otherwise
data['Abnormal_URL'] = data['url'].apply(abnormal_url)

data.head()

sns.set_theme(style="darkgrid")
sns.countplot(x='Abnormal_URL', data=data, palette="bright")
plt.xlabel('Abnormal URL')
plt.ylabel('Count')
plt.show()

# Create a crosstab and plot the bar chart
pd.crosstab(data["Abnormal_URL"], data["type"]).plot(kind="bar",figsize=(12,5),color=['#003f5c','#ffa600','#bc5090','#ff6361'])
plt.title('Distribution based on Abnormal_URL and type')
plt.xlabel('Abnormal_URL')
plt.xticks(rotation=0)
plt.ylabel('Frequency')
plt.legend(title="type")
plt.show()

# Function to detect if the URL has "https"
def has_https(url):
    return int("https" in url)

# Add a new 'Has_HTTPS' column with 1 if there's "https", 0 otherwise
data['Has_HTTPS'] = data['url'].apply(has_https)

# Count the occurrences of each value in the "Has_HTTPS" column
https_counts = data['Has_HTTPS'].value_counts()

# Plot a pie chart
plt.figure(figsize=(6, 6))
plt.pie(https_counts, labels=https_counts.index, autopct='%1.1f%%', colors=sns.color_palette("pastel"))
plt.title('Distribution of HTTPS Usage')
plt.show()

# Create a bar plot using Seaborn
plt.figure(figsize=(10, 6))
sns.countplot(data=data, x='type', hue='Has_HTTPS',palette="bright")
plt.title("Relation between Type and Has_HTTPS")
plt.xlabel("Type")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.legend(title="Has HTTPS")
plt.tight_layout()
plt.show()

# Function to count the number of digits in a string
def count_digits(string):
    return sum(1 for char in string if char.isdigit())

# Function to count the number of letters in a string
def count_letters(string):
    return sum(1 for char in string if char.isalpha())

# Add new columns for digit and letter counts
data['Digit_Count'] = data['url'].apply(count_digits)
data['Letter_Count'] = data['url'].apply(count_letters)

# Create a scatter plot using Seaborn
plt.figure(figsize=(10, 6))
sns.scatterplot(data=data, x='type', y='Digit_Count')
plt.title("Relation between Number of Digits and Type")
plt.xlabel("Type")
plt.ylabel("Number of Digits")
plt.tight_layout()
plt.show()

# Create a scatter plot using Seaborn
plt.figure(figsize=(10, 6))
sns.scatterplot(data=data, x='type', y='Letter_Count')
plt.title("Relation between Number of Letters and Type")
plt.xlabel("Type")
plt.ylabel("Number of Letters")
plt.tight_layout()
plt.show()

# Define the regular expression pattern for shortening services
shortening_pattern = r'bit\.ly|goo\.gl|shorte\.st|go2l\.ink|x\.co|ow\.ly|t\.co|tinyurl|tr\.im|is\.gd|cli\.gs|' \
                     r'yfrog\.com|migre\.me|ff\.im|tiny\.cc|url4\.eu|twit\.ac|su\.pr|twurl\.nl|snipurl\.com|' \
                     r'short\.to|BudURL\.com|ping\.fm|post\.ly|Just\.as|bkite\.com|snipr\.com|fic\.kr|loopt\.us|' \
                     r'doiop\.com|short\.ie|kl\.am|wp\.me|rubyurl\.com|om\.ly|to\.ly|bit\.do|t\.co|lnkd\.in|' \
                     r'db\.tt|qr\.ae|adf\.ly|goo\.gl|bitly\.com|cur\.lv|tinyurl\.com|ow\.ly|bit\.ly|ity\.im|' \
                     r'q\.gs|is\.gd|po\.st|bc\.vc|twitthis\.com|u\.to|j\.mp|buzurl\.com|cutt\.us|u\.bb|yourls\.org|' \
                     r'x\.co|prettylinkpro\.com|scrnch\.me|filoops\.info|vzturl\.com|qr\.net|1url\.com|tweez\.me|v\.gd|' \
                     r'tr\.im|link\.zip\.net'

# Function to detect if the URL uses a shortening service
def has_shortening_service(url):
    return int(re.search(shortening_pattern, url, flags=re.I) is not None)

# Add a new 'Has_Shortening_Service' column with 1 if it has a shortening service, 0 otherwise
data['Has_Shortening_Service'] = data['url'].apply(has_shortening_service)

plt.figure(figsize=(10, 6))
sns.countplot(data=data, x='type', hue='Has_Shortening_Service',palette='bright')
plt.title("Relation between Type and Has_Shortening_Service")
plt.xlabel("Type")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.legend(title="Has Shortening Service")
plt.tight_layout()
plt.show()

data.head()

# Define the regular expression pattern to match various IP address formats
ip_pattern = (
    r'(([01]?\d\d?|2[0-4]\d|25[0-5])\.([01]?\d\d?|2[0-4]\d|25[0-5])\.([01]?\d\d?|2[0-4]\d|25[0-5])\.'
    r'([01]?\d\d?|2[0-4]\d|25[0-5])\/)|'
    r'(([01]?\d\d?|2[0-4]\d|25[0-5])\.([01]?\d\d?|2[0-4]\d|25[0-5])\.([01]?\d\d?|2[0-4]\d|25[0-5])\.'
    r'([01]?\d\d?|2[0-4]\d|25[0-5])\/)|'
    r'((0x[0-9a-fA-F]{1,2})\.(0x[0-9a-fA-F]{1,2})\.(0x[0-9a-fA-F]{1,2})\.(0x[0-9a-fA-F]{1,2})\/)'
    r'(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}|'
    r'([0-9]+(?:\.[0-9]+){3}:[0-9]+)|'
    r'((?:(?:\d|[01]?\d\d|2[0-4]\d|25[0-5])\.){3}(?:25[0-5]|2[0-4]\d|[01]?\d\d|\d)(?:\/\d{1,2})?)'
)

# Function to detect if the URL has an IP address
def has_ip_address(url):
    return int(re.search(ip_pattern, url, flags=re.I) is not None)

# Add a new 'Has_IP_Address' column with 1 if it has an IP address, 0 otherwise
data['Has_IP_Address'] = data['url'].apply(has_ip_address)

# Create a bar plot using Seaborn
plt.figure(figsize=(10, 6))
sns.countplot(data=data, x='type', hue='Has_IP_Address',palette='bright')
plt.title("Relation between Type and Has_IP_Address")
plt.xlabel("Type")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.legend(title="Has IP Address")
plt.tight_layout()
plt.show()

data.head()

# Function to check for malicious code in a URL
def check_for_malicious_code(url):
    # Check for 'javascript:' in the URL
    if re.search(r'javascript:', url):
        return 1

    # Check for attempts to inject script or use 'on' attributes
    if re.search(r'<\s*script', url, re.IGNORECASE) or re.search(r'on\w*=', url, re.IGNORECASE):
        return 1

    return 0

# Add a new 'Has_Malicious_Code' column with 1 if it has malicious code, 0 otherwise
data['Has_javascript_Code'] = data['url'].apply(check_for_malicious_code)

# Create a crosstab and plot the bar chart
pd.crosstab(data["Has_javascript_Code"], data["type"]).plot(kind="bar", figsize=(12, 5),
                                                           color=['#003f5c', '#ffa600', '#bc5090', '#ff6361'])
plt.title('Distribution based on Has_javascript_Code and type')
plt.xlabel('Has_javascript_Code')
plt.xticks(rotation=0)
plt.ylabel('Frequency')
plt.legend(title="type")
plt.show()

data.head()

# Define the function to check text encoding
def check_text_encoding(url):
    # Parse the URL
    parsed_url = urlparse(url)

    # Extract the text part
    text_part = parsed_url.path

    # Check for encoding
    decoded_text = unquote(text_part)

    # Check if the decoded text matches the original text
    if decoded_text == text_part:
        return 0  # No encoding found
    else:
        return 1  # Encoding found

# Apply the function to the 'url' column and create a new column 'Has_Text_Encoding'
data['Has_Text_Encoding'] = data['url'].apply(check_text_encoding)

data.head()

# Create a bar plot using Seaborn
plt.figure(figsize=(10, 6))
sns.countplot(data=data, x='type', hue='Has_Text_Encoding',palette='bright')
plt.title("Relation between Type and Has_Text_Encoding")
plt.xlabel("Type")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.legend(title="Has Text Encoding")
plt.tight_layout()
plt.show()

# 从原始数据框（DataFrame）data中选择只包含数值类型（如整数、浮点数等）的列。
data = data.select_dtypes(include=[np.number])

# Calculate correlations between columns
correlation_matrix = data.corr()

# Create a heatmap using Seaborn
plt.figure(figsize=(20, 15))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("Correlation Heatmap")
plt.show()

# Select the top 10 features related to the "Category" column
top_features = data.corr().abs()['Category'].sort_values(ascending=False)[1:11].index

# Calculate correlations for the selected features
correlation_matrix = data[top_features].corr()

# Create a heatmap using Seaborn
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("Top 10 Features Correlation Heatmap with Category")
plt.show()

data.isnull().sum()

X = data.drop(['Category','type_ratio','*'],axis=1)#,'type_code'
y = data['Category']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Print the shapes of the resulting sets
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

data.columns

# Print the shapes of the resulting sets
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

from sklearn.metrics import confusion_matrix
import itertools
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    See full source and example:
    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html

    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

"""# hypermeters tuning"""

'''
import optuna
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

def objective(trial):
    # Number of trees in random forest
    n_estimators = trial.suggest_int(name="n_estimators", low=100, high=500, step=100)

    # Number of features to consider at every split
    max_features = trial.suggest_categorical(name="max_features", choices=['auto', 'sqrt'])

    # Maximum number of levels in tree
    max_depth = trial.suggest_int(name="max_depth", low=10, high=110, step=20)

    # Minimum number of samples required to split a node
    min_samples_split = trial.suggest_int(name="min_samples_split", low=2, high=10, step=2)

    # Minimum number of samples required at each leaf node
    min_samples_leaf = trial.suggest_int(name="min_samples_leaf", low=1, high=4, step=1)

    params = {
        "n_estimators": n_estimators,
        "max_features": max_features,
        "max_depth": max_depth,
        "min_samples_split": min_samples_split,
        "min_samples_leaf": min_samples_leaf
    }

    model = RandomForestClassifier(**params)
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    # Calculate the accuracy of the model
    accuracy = accuracy_score(y_test, predictions)
    return accuracy

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=30)

print('Best hyperparameters:', study.best_params)
print('Best Accuracy:', study.best_value)
'''

import time
from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc
from sklearn.metrics import confusion_matrix

# Initialize and start the timer for model training
start_time = time.time()
# Initialize and train the RandomForestClassifier
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)
# Calculate the model training time
training_time_rf = time.time() - start_time
print("RandomForestClassifier Training Time:", training_time_rf, "seconds")

# Initialize and start the timer for model execution
start_time = time.time()
# Predict the labels for the testing set
y_pred_RF = clf.predict(X_test)
# Calculate the model execution time
execution_time_rf = time.time() - start_time
print("RandomForestClassifier Execution Time:", execution_time_rf, "seconds")

cm = confusion_matrix(y_test, y_pred_RF)
cm_df = pd.DataFrame(cm,
                     index = ['benign', 'defacement', 'phishing', 'malware'],
                     columns = ['benign', 'defacement', 'phishing', 'malware'])
plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True, cmap='coolwarm', linewidths=0.5, fmt=".0f")
plt.title('RandomForestClassifier Confusion Matrix')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred_RF)
print("RandomForestClassifier Accuracy:", accuracy)
# Calculate the precision, recall, and F1-score
precision = precision_score(y_test, y_pred_RF, average='macro')
recall = recall_score(y_test, y_pred_RF, average='macro')
f1 = f1_score(y_test, y_pred_RF, average='macro')
print("RandomForestClassifier Precision:", precision)
print("RandomForestClassifier Recall:", recall)
print("RandomForestClassifier F1-score:", f1)


# from sklearn.metrics import plot_roc_curve
# 绘制 ROC 曲线和计算 AUC
# plot_roc_curve(clf, X_test, y_test)
# plt.show()

sample_weight = (y_pred_RF != y_test)
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_RF, sample_weight=sample_weight, normalize="true", values_format=".0%")
plt.title('Random Forest Classifier Error Matrix')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()

from xgboost import XGBClassifier

# Initialize and start the timer for model training
start_time = time.time()
# Initialize and train the XGBClassifier
XGB_Classifier = XGBClassifier(random_state=42)
XGB_Classifier.fit(X_train, y_train)
# Calculate the model training time
training_time_XGB = time.time() - start_time
print("XGBClassifier Training Time:", training_time_XGB, "seconds")

# Initialize and start the timer for model execution
start_time = time.time()
# Predict the labels for the testing set
y_pred_XGB = XGB_Classifier.predict(X_test)
# Calculate the model execution time
execution_time_xgb = time.time() - start_time
print("XGBClassifier Execution Time:", execution_time_xgb, "seconds")

cm = confusion_matrix(y_test, y_pred_XGB)
cm_df = pd.DataFrame(cm,
                     index = ['benign', 'defacement', 'phishing', 'malware'],
                     columns = ['benign', 'defacement', 'phishing', 'malware'])
plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True, cmap='coolwarm', linewidths=0.5, fmt=".0f")
plt.title('XGBClassifier Confusion Matrix')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred_XGB)
print("XGBClassifier Accuracy:", accuracy)
# Calculate the precision, recall, and F1-score
precision = precision_score(y_test, y_pred_XGB, average='macro')
recall = recall_score(y_test, y_pred_XGB, average='macro')
f1 = f1_score(y_test, y_pred_XGB, average='macro')
print("XGBClassifier Precision:", precision)
print("XGBClassifier Recall:", recall)
print("XGBClassifier F1-score:", f1)

sample_weight = (y_pred_XGB != y_test)
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_XGB, sample_weight=sample_weight, normalize="true", values_format=".0%")
plt.title('XGBClassifier Error Matrix')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()

import time

# Initialize and start the timer for model training
start_time_ada = time.time()
# Initialize and train the AdaBoostClassifier
ada_classifier = AdaBoostClassifier(random_state=42)
ada_classifier.fit(X_train, y_train)
# Calculate the model training time
training_time_ada = time.time() - start_time_ada
print("AdaBoostClassifier Training Time:", training_time_ada, "seconds")

# Initialize and start the timer for model execution
start_time = time.time()
# Predict the labels for the testing set
y_pred_ada = ada_classifier.predict(X_test)
# Calculate the model execution time
execution_time_ada = time.time() - start_time
print("AdaBoostClassifier Execution Time:", execution_time_ada, "seconds")

cm = confusion_matrix(y_test, y_pred_ada)
cm_df = pd.DataFrame(cm,
                     index = ['benign', 'defacement', 'phishing', 'malware'],
                     columns = ['benign', 'defacement', 'phishing', 'malware'])
plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True, cmap='coolwarm', linewidths=0.5, fmt=".0f")
plt.title('AdaBoostClassifier Confusion Matrix')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()

# Calculate the accuracy of the AdaBoost model
accuracy_ada = accuracy_score(y_test, y_pred_ada)
print("AdaBoost Accuracy:", accuracy_ada)
# Calculate the precision, recall, and F1-score
precision = precision_score(y_test, y_pred_ada, average='macro')
recall = recall_score(y_test, y_pred_ada, average='macro')
f1 = f1_score(y_test, y_pred_ada, average='macro')
print("AdaBoost Precision:", precision)
print("AdaBoost Recall:", recall)
print("AdaBoost F1-score:", f1)

sample_weight = (y_pred_ada != y_test)
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_ada, sample_weight=sample_weight, normalize="true", values_format=".0%")
plt.title('AdaBoostClassifier Error Matrix')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
# Save the plot as a PDF
plt.savefig('AdaBoost Classifier Error Matrix.pdf', format='pdf')
plt.show()

# Initialize and start the timer for model training
start_time = time.time()
# Initialize and train the SGDClassifier
sgd_classifier = SGDClassifier(random_state=42)
sgd_classifier.fit(X_train, y_train)
# Calculate the model training time
training_time_sgd = time.time() - start_time
print("SGD Classifier Training Time:", training_time_sgd, "seconds")

# Initialize and start the timer for model execution
start_time = time.time()
# Predict the labels for the testing set
y_pred_sgd = sgd_classifier.predict(X_test)
# Calculate the model execution time
execution_time_sgd = time.time() - start_time
print("SGD Classifier Execution Time:", execution_time_sgd, "seconds")

cm = confusion_matrix(y_test, y_pred_sgd)
cm_df = pd.DataFrame(cm,
                     index = ['benign', 'defacement', 'phishing', 'malware'],
                     columns = ['benign', 'defacement', 'phishing', 'malware'])
plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True, cmap='coolwarm', linewidths=0.5, fmt=".0f")
plt.title('SGDClassifier Confusion Matrix')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
# Save the plot as a PDF
plt.savefig('SGD Classifier Confusion Matrix.pdf', format='pdf')
plt.show()

# Calculate the accuracy of the SGD model
accuracy_sgd = accuracy_score(y_test, y_pred_sgd)
print("SGD Classifier Accuracy:", accuracy_sgd)
# Calculate the precision, recall, and F1-score
precision = precision_score(y_test, y_pred_sgd, average='macro')
recall = recall_score(y_test, y_pred_sgd, average='macro')
f1 = f1_score(y_test, y_pred_sgd, average='macro')
print("SGD Classifier Precision:", precision)
print("SGD Classifier Recall:", recall)
print("SGD Classifier F1-score:", f1)

sample_weight = (y_pred_sgd != y_test)
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_sgd, sample_weight=sample_weight, normalize="true", values_format=".0%")
plt.title('SGD Classifier Error Matrix')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')

# Save the plot as a PDF
plt.savefig('my_plot.pdf', format='pdf')
plt.show()
# It's good practice to close the plot if you're done with it, especially when running scripts.
plt.close()



# Initialize and start the timer for model training
start_time = time.time()
# Initialize and train the ExtraTreesClassifier
extra_trees_classifier = ExtraTreesClassifier(random_state=42)
extra_trees_classifier.fit(X_train, y_train)
# Calculate the model training time
training_time_extra = time.time() - start_time
print("ExtraTrees Classifier Training Time:", training_time_extra, "seconds")

# Initialize and start the timer for model execution
start_time = time.time()
# Predict the labels for the testing set
y_pred_extra_trees = extra_trees_classifier.predict(X_test)
# Calculate the model execution time
execution_time_extra = time.time() - start_time
print("ExtraTrees Classifier Execution Time:", execution_time_extra, "seconds")

cm = confusion_matrix(y_test, y_pred_extra_trees)
cm_df = pd.DataFrame(cm,
                     index = ['benign', 'defacement', 'phishing', 'malware'],
                     columns = ['benign', 'defacement', 'phishing', 'malware'])
plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True, cmap='coolwarm', linewidths=0.5, fmt=".0f")
plt.title('ExtraTrees Classifier Confusion Matrix')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()

# Calculate the accuracy of the ExtraTrees model
accuracy_extra_trees = accuracy_score(y_test, y_pred_extra_trees)
print("ExtraTrees Classifier Accuracy:", accuracy_extra_trees)

# Calculate the precision, recall, and F1-score
precision = precision_score(y_test, y_pred_extra_trees, average='macro')
recall = recall_score(y_test, y_pred_extra_trees, average='macro')
f1 = f1_score(y_test, y_pred_extra_trees, average='macro')

print("ExtraTrees Classifier Precision:", precision)
print("ExtraTrees Classifier Recall:", recall)
print("ExtraTrees Classifier F1-score:", f1)

sample_weight = (y_pred_extra_trees != y_test)
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_extra_trees, sample_weight=sample_weight, normalize="true", values_format=".0%")
plt.title('ExtraTrees Classifier Error Matrix')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()

"""# Ensemble Learning"""

from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score
# Initialize and start the timer for model training
start_time = time.time()
from sklearn.ensemble import GradientBoostingClassifier
gbdt = GradientBoostingClassifier(n_estimators=100,max_features='sqrt')
gbdt.fit(X_train,y_train)
# Calculate the model training time
training_time_Gbc = time.time() - start_time
print("Gradient Boosting Classifier Training Time:", training_time_Gbc, "seconds")

# Initialize and start the timer for model execution
start_time = time.time()
y_pred_gbc = gbdt.predict(X_test)
# Calculate the model execution time
execution_time_gbc = time.time() - start_time
print("Gradient Boosting Classifier Execution Time:", execution_time_gbc, "seconds")

print(classification_report(y_test,y_pred_gbc))

score = accuracy_score(y_test, y_pred_gbc)
print("accuracy:   %0.3f" % score)

from sklearn.metrics import ConfusionMatrixDisplay
cm = confusion_matrix(y_test, y_pred_gbc)
cm_df = pd.DataFrame(cm,
                     index = ['benign', 'defacement', 'phishing', 'malware'],
                     columns = ['benign', 'defacement', 'phishing', 'malware'])
plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True, cmap='coolwarm', linewidths=0.5, fmt=".0f")
plt.title('Gradient Boosting Classifier Confusion Matrix')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()

sample_weight = (y_pred_gbc != y_test)
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_gbc, sample_weight=sample_weight, normalize="true", values_format=".0%")
plt.title('Gradient Boosting Classifier Error Matrix')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()

"""# Performance Report"""

from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score

model_names = ['SGD Classifier', 'AdaBoostClassifier', 'ExtraTrees Classifier', 'XGBClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier']

# Assuming you have imported the classifiers and defined their predictions
classifiers = [y_pred_sgd, y_pred_ada, y_pred_extra_trees, y_pred_XGB, y_pred_RF, y_pred_gbc]
# model_names = ['SGD Classifier', 'AdaBoostClassifier', 'ExtraTrees Classifier', 'XGBClassifier', 'RandomForestClassifier']

# Create a dictionary to map model names to their respective predictions
classifier_map = dict(zip(model_names, classifiers))

# Now you can use this dictionary to iterate over each model name and its predictions
for model_name, y_pred in classifier_map.items():
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='macro')
    recall = recall_score(y_test, y_pred, average='macro')
    f1score = f1_score(y_test, y_pred, average='macro')
    class_report = classification_report(y_test, y_pred)

    print(f"\nMetrics for Model '{model_name}':")
    print(f"Accuracy: {accuracy}")
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"F1-score: {f1score}")
    print(f"Classification Report:\n{class_report}")

import numpy as np
import matplotlib.pyplot as plt

# Initialize data for the bar chart
models = list(classifier_map.keys())
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']

# Calculate metric scores for each model
metric_scores = {}
for metric in metrics:
    metric_scores[metric] = [accuracy_score(y_test, classifier_map[model_name]) if metric == 'Accuracy'
                             else precision_score(y_test, classifier_map[model_name], average='macro') if metric == 'Precision'
                             else recall_score(y_test, classifier_map[model_name], average='macro') if metric == 'Recall'
                             else f1_score(y_test, classifier_map[model_name], average='macro') for model_name in models]

# Set width of bars
bar_width = 0.2

# Set position of bars on X axis
r1 = np.arange(len(models))
r2 = [x + bar_width for x in r1]
r3 = [x + bar_width for x in r2]
r4 = [x + bar_width for x in r3]

plt.figure(figsize=(20, 8))

# Define light colors
colors = ['#48C9B0', '#FFA500', '#FF6347', '#7B68EE']

# Plotting the bars
bars = []
bars.append(plt.bar(r1, metric_scores['Accuracy'], color=colors[0], width=bar_width, edgecolor='grey', label='Accuracy'))
bars.append(plt.bar(r2, metric_scores['Precision'], color=colors[1], width=bar_width, edgecolor='grey', label='Precision'))
bars.append(plt.bar(r3, metric_scores['Recall'], color=colors[2], width=bar_width, edgecolor='grey', label='Recall'))
bars.append(plt.bar(r4, metric_scores['F1-score'], color=colors[3], width=bar_width, edgecolor='grey', label='F1-score'))

# Adding labels on top of each bar
for i, container in enumerate(bars):
    for j, bar in enumerate(container):
        yval = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2.0, yval, f'{yval:.2f}', va='bottom')

# Add xticks on the middle of the group bars
plt.xlabel('Models', fontweight='bold')
plt.xticks([r + bar_width*1.5 for r in range(len(models))], models)

# Set y-axis limits
plt.ylim(0.3, 1.0)

# Add a legend and show the plot
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Assuming you have imported the classifiers and defined their predicting times
execution_times = [execution_time_sgd, execution_time_ada, execution_time_extra, execution_time_xgb, execution_time_rf, execution_time_gbc]
model_names = ['SGD Classifier', 'AdaBoostClassifier', 'ExtraTrees Classifier', 'XGBClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier']

# Create a dictionary to map model names to their execution_times (optional since we directly use execution_times and model_names)
# execution_times_map = dict(zip(model_names, execution_times))

# Initialize data for the bar chart
execution_models = model_names  # Directly using model_names for clarity
execution_metrics = ['Execution Time']

# Set width of bars
bar_width = 0.2

# Set position of bars on X axis
r1 = np.arange(len(execution_models))

plt.figure(figsize=(13, 7))

# Define light colors
colors = ['#48C9B0', '#FFA500', '#FF6347', '#7B68EE', '#8A2BE2']  # Added another color for the fifth bar

# Plotting the bars
bars = plt.bar(r1, execution_times, color=colors, width=bar_width, edgecolor='grey')

# Adding labels on top of each bar
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval, f'{yval:.2f}', va='bottom')  # Adjust va to 'bottom' to place text above bar

# Add xticks on the middle of the group bars (correcting variable name)
plt.xlabel('Models', fontweight='bold')
plt.xticks([r + bar_width * 0.5 for r in r1], execution_models)  # Center alignment

# Set y-axis limits
plt.ylim(0, max(execution_times) * 1.1)  # Dynamic adjustment for y-axis limit

# Add a legend and show the plot
plt.ylabel('Execution Time (seconds)')
plt.legend()
plt.tight_layout()  # Adjust layout for better fit
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Assuming you have imported the classifiers and defined their training times
training_times = [training_time_sgd, training_time_ada, training_time_extra, training_time_XGB, training_time_rf, training_time_Gbc]

# Create a dictionary to map model names to their execution_times (this step might not be necessary for just plotting)
# training_times_map = dict(zip(model_names, training_times))  # This line can be omitted if we directly use training_models

# Initialize data for the bar chart
training_models = ['SGD', 'AdaBoost', 'ExtraTrees', 'XGBoost', 'RandomForest', 'GrandientBoosting']  # Assuming these are your model names
training_metrics = ['Training Time']

# Set width of bars
bar_width = 0.2

# Set position of bars on X axis
r1 = np.arange(len(training_models))

plt.figure(figsize=(13, 7))

# Define light colors
colors = ['#48C9B0', '#FFA500', '#FF6347', '#7B68EE', '#8A2BE2']

# Plotting the bars with direct labels
bars = plt.bar(r1, training_times, color=colors, width=bar_width, edgecolor='grey')

# Adding labels on top of each bar
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval, f'{yval:.2f}', va='bottom')  # Adjust va to 'bottom' to place text above bar

# Add xticks on the middle of the group bars
plt.xlabel('Models', fontweight='bold')
plt.xticks([r + bar_width*0.5 for r in r1], training_models)  # Adjusted for center alignment

# Set y-axis limits
plt.ylim(0, max(training_times)*1.1)  # Adjust dynamically based on data

# Add a legend and show the plot
plt.ylabel('Training Time (seconds)')
plt.legend()
plt.tight_layout()  # Adjust layout to fit everything nicely
plt.show()